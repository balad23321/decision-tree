{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-7bf20e901374>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-7bf20e901374>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    of all entropy based on a specific split ○ The higher the information gain, the better the decision split is\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def entropy(s): \n",
    "    counts = np.bincount(s) \n",
    "    percentages = counts / len(s)\n",
    "\n",
    "    entropy = 0 \n",
    "    for pct in percentages: \n",
    "        if pct > 0: \n",
    "            entropy += pct * np.log2(pct) \n",
    "    return -entropy\n",
    "\n",
    "s = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1] \n",
    "print(f'Entropy: {np.round(entropy(s), 5)}')\n",
    "Entropy: 0.88129 \n",
    "    of all entropy based on a specific split ○ The higher the information gain, the better the decision split is\n",
    "\n",
    "def information_gain(parent, left_child, right_child): \n",
    "    num_left = len(left_child) / len(parent) \n",
    "    num_right = len(right_child) / len(parent)\n",
    "gain = entropy(parent) - (num_left * entropy(left_child) + num_right * entropy(right_child)) \n",
    "return gain\n",
    "\n",
    "parent = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1] left_child = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1] right_child = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "print(f'Information gain:{np.round(information_gain(parent, left_child, right_child), 5)}')\n",
    "\n",
    "Information gain: 0.18094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursion Crash Course\n",
    "\n",
    "def factorial(x): \n",
    "    # Exit condition \n",
    "    if x == 1: \n",
    "        return 1 \n",
    "    return x * factorial(x - 1)\n",
    "\n",
    "    print(f'Factorial of 5 is {factorial(5)}')\n",
    "\n",
    "    Factorial of 5 is 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None): \n",
    "    self.feature = feature \n",
    "    self.threshold = threshold \n",
    "    self.data_left = data_left \n",
    "    self.data_right = data_right \n",
    "    self.gain = gain \n",
    "    self.value = value \n",
    "    \n",
    "classDecisionTree:\n",
    "\n",
    "def __init__(self, min_samples_split=2, max_depth=5): \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.max_depth = max_depth \n",
    "        self.root = None\n",
    "\n",
    "def _entropy(s): \n",
    "    # Convert to integers to avoid runtime errors \n",
    "    counts = np.bincount(np.array(s, dtype=np.int64)) \n",
    "    # Probabilities of each class label percentages = counts / len(s)\n",
    "# Caclulate entropy\n",
    "entropy = 0 for \n",
    "pct in percentages: \n",
    "    if pct > 0: \n",
    "        entropy += pct * np.log2(pct) \n",
    "    return -entropy\n",
    "    \n",
    "def _information_gain(self, parent, left_child, right_child) \n",
    "\n",
    "\n",
    "     num_left = len(left_child) / len(parent) \n",
    "     num_right = len(right_child) / len(parent)\n",
    "        \n",
    "# One-liner which implements the previously discussed formula \n",
    "\n",
    "return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * \n",
    "self._entropy(right_child))\n",
    "\n",
    "def _best_split(self, X, y): \n",
    "     \n",
    "best_split = {} \n",
    "best_info_gain = -1 \n",
    "n_rows, n_cols = X.shape\n",
    "\n",
    "# For every dataset feature \n",
    "\n",
    "for f_idx in range(n_cols): \n",
    "    X_curr = X[:, f_idx]\n",
    "    \n",
    "    # For every unique value of that feature \n",
    "    for threshold in np.unique(X_curr): \n",
    "        # Construct a dataset and split it to the left and right parts \n",
    "        # Left part includes records lower or equal to the threshold \n",
    "        # Right part includes records higher than the threshold \n",
    "    df = np.concatenate((X, y.reshape(1, -1).T), axis=1) \n",
    "    df_left = np.array([row for row in df if row[f_idx] <= threshold]) \n",
    "    df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
    "    \n",
    "# Do the calculation only if there's data in both subsets \n",
    "if len(df_left) > 0 and len(df_right) > 0: \n",
    "    # Obtain the value of the target variable for subsets \n",
    "    y = df[:, -1] \n",
    "    y_left = df_left[:, -1] \n",
    "    y_right = df_right[:, -1]\n",
    "    \n",
    "# Caclulate the information gain and save the split parameters \n",
    "# if the current split if better then the previous best \n",
    "gain = self._information_gain(y, y_left, y_right)\n",
    "if gain > best_info_gain:\n",
    "    \n",
    "    best_split = { \n",
    "        'feature_index': f_idx, \n",
    "        'threshold': threshold, 'df_left': df_left, \n",
    "        'df_right': df_right, \n",
    "        'gain': gain } \n",
    "    best_info_gain = gain \n",
    "    return best_split\n",
    "\n",
    "def _build(self, X, y, depth=0):\n",
    "    \n",
    ":param X: np.array, features \n",
    "        :param y: np.array or list, target \n",
    "                :param depth: current depth of a tree, used as a stopping creteria\n",
    "                :return:Node       \n",
    "\n",
    "n_rows, n_cols = X.shape\n",
    "\n",
    "# Check to see if a node should be leaf node \n",
    "if n_rows >= self.min_samples_split and depth <= self.max_depth: \n",
    "    # Get the best split \n",
    "    best = self._best_split(X, y) \n",
    "    # If the split isn't pure \n",
    "    if best['gain'] > 0: \n",
    "    # Build a tree on the left \n",
    "    left = self._build\n",
    "    X=best['df_left'][:, :-1], \n",
    "    y=best['df_left'][:, -1], \n",
    "    depth=depth + 1 \n",
    "    ) \n",
    "    right = self._build\n",
    "     X=best['df_right'][:, :-1], \n",
    "     y=best['df_right'][:, -1], \n",
    "     depth=depth + 1 ) \n",
    "    return Node( feature=best['feature_index'], \n",
    "                threshold=best['threshold'], \n",
    "                data_left=left, \n",
    "                data_right=right, \n",
    "                gain=best['gain']\n",
    "               ) \n",
    "# Leaf node - value is the most common target value\n",
    "\n",
    "return Node\n",
    "( value=Counter(y).most_common(1)[0][0] \n",
    ")\n",
    "\n",
    "def fit(self, X, y):  \n",
    "    \n",
    "    # Call a recursive function to build the tree \n",
    "    self.root = self._build(X, y)\n",
    "\n",
    "def _predict(self, x, tree):  \n",
    "    \n",
    "    # Leaf node \n",
    "if tree.value != None: \n",
    "        return tree.value \n",
    "    feature_value = x[tree.feature]\n",
    "    \n",
    "# Go to the left \n",
    "if feature_value <= tree.threshold: \n",
    "    return self._predict(x=x, tree=tree.data_left)\n",
    "\n",
    "# Go to the right \n",
    "if feature_value > tree.threshold: \n",
    "    return self._predict(x=x, tree=tree.data_right)\n",
    "def predict(self, X):\n",
    "\n",
    "# Call the _predict() function for every observation \n",
    "return [self._predict(x, self.root) for x in X]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris['data'] \n",
    "y = iris['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = DecisionTree() \n",
    "    model.fit(X_train, y_train) \n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "np.array(preds, dtype=np.int64)\n",
    "\n",
    "y_test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparism\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "sk_model = DecisionTreeClassifier() \n",
    "sk_model.fit(X_train, y_train) \n",
    "sk_preds = sk_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, sk_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
